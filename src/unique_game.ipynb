{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.asyncio import tqdm\n",
    "import aiohttp\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s) :\n",
    "    new_str = s.lower()\n",
    "    new_str = new_str.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_str = new_str.translate(str.maketrans('', '', string.digits))\n",
    "    return new_str.strip()\n",
    "\n",
    "def clean_names(df) :\n",
    "    def clean_name(row):\n",
    "        return clean_text(row['name'])\n",
    "    new_df = df.copy()\n",
    "    new_df['name'] = new_df.apply(clean_name,axis=1)\n",
    "    return new_df\n",
    "    \n",
    "\n",
    "def clean_description(df):\n",
    "    def clean_desc(row):\n",
    "        desc = row['short_description']\n",
    "        if isinstance(desc, str):\n",
    "            return clean_text(desc)\n",
    "        else:\n",
    "            return ''\n",
    "    def clean_detailed_desc(row):\n",
    "        desc = row['detailed_description']\n",
    "        if isinstance(desc, str):\n",
    "            return clean_text(desc)\n",
    "        else:\n",
    "            return ''\n",
    "    new_df = df.copy()\n",
    "    new_df['short_description'] = new_df.apply(clean_desc, axis=1)\n",
    "    new_df['detailed_description'] = new_df.apply(clean_detailed_desc, axis=1)\n",
    "    return new_df\n",
    "def parse_and_clean_tags(tag_string):\n",
    "    \"\"\"\n",
    "    Parses a comma-separated string of tags and cleans each one.\n",
    "    \"\"\"\n",
    "    if not isinstance(tag_string, str):\n",
    "        return []\n",
    "    \n",
    "    # Split the string into individual tags by the comma\n",
    "    tags = tag_string.split(',')\n",
    "    \n",
    "    cleaned_tags = []\n",
    "    for tag in tags:\n",
    "        # Now clean each individual tag\n",
    "        tag = tag.lower()\n",
    "        tag = tag.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "        tag = tag.strip()\n",
    "        if tag: # Make sure the tag isn't empty\n",
    "            cleaned_tags.append(tag)\n",
    "            \n",
    "    return cleaned_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  36%|███▌      | 602/1663 [06:07<09:46,  1.81it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "steam = pd.read_csv(\"../data/updated_steam_games.csv\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "steam = clean_names(steam)\n",
    "steam = clean_description(steam)\n",
    "\n",
    "#steam = steam.head(5000)\n",
    "steam = steam[steam['pct_pos_total'] > -1]\n",
    "\n",
    "all_tags = set()\n",
    "for tag_string in steam['tags'].dropna():\n",
    "    tags = parse_and_clean_tags(tag_string)\n",
    "    all_tags.update(tags)\n",
    "\n",
    "unique_tags_list = list(all_tags)\n",
    "\n",
    "\n",
    "steam['combined_text'] = steam['short_description'] + ' ' + steam['tags'] + ' ' + steam['detailed_description']\n",
    "\n",
    "transformed_output = model.encode(steam['combined_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "tag_embeddings = model.encode(unique_tags_list, show_progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit it on your reference corpus (all the tags) and transform them.\n",
    "# This is now your reference matrix of embeddings.\n",
    "tag_embeddings = vectorizer.fit_transform(unique_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 closest games:\n",
      "- autoheroes: 0.6261\n",
      "- auto chess: 0.6187\n",
      "- behold battle: 0.6037\n",
      "- tiny battles: 0.6037\n",
      "- auto riskrisk: 0.5962\n",
      "Unique score: 0.37391865\n"
     ]
    }
   ],
   "source": [
    "new_desc = \"auto battler\"\n",
    "new_desc = clean_text(new_desc)\n",
    "new_vector = model.encode([new_desc])\n",
    "\n",
    "\n",
    "\n",
    "similarity_scores = cosine_similarity(new_vector,transformed_output)\n",
    "\n",
    "top_5_indices = np.argsort(similarity_scores[0])[-5:][::-1]\n",
    "\n",
    "\n",
    "# 7. Print the Top 5 Games and their Scores\n",
    "print(\"Top 5 closest games:\")\n",
    "for i in top_5_indices:\n",
    "  game_name = steam.iloc[i]['name']\n",
    "  score = similarity_scores[0][i]\n",
    "  print(f\"- {game_name}: {score:.4f}\")\n",
    "  \n",
    "closest_match_similarity = np.max(similarity_scores[0])\n",
    "uniqueness_score = 1 - closest_match_similarity\n",
    "print(\"Unique score:\", uniqueness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tags for the description:\n",
      "- auto battler: (Score: 1.0000)\n",
      "- card battler: (Score: 0.4951)\n",
      "- fps: (Score: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def get_tags_for_description(description, tags_list, tag_embeddings, vectorizer, top_n=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant tags for a given description.\n",
    "    The vectorizer should already be fitted on the tag corpus.\n",
    "    \"\"\"\n",
    "    # 1. Use the PRE-FITTED vectorizer to transform the new description.\n",
    "    #    Note: We use .transform(), NOT .fit_transform()\n",
    "    description_embedding = vectorizer.transform([description])\n",
    "\n",
    "    # 2. Calculate cosine similarity between the description and all tags\n",
    "    sim_scores = cosine_similarity(description_embedding, tag_embeddings)\n",
    "\n",
    "    # 3. Get the top N scores and their indices\n",
    "    top_indices = np.argsort(sim_scores[0])[-top_n:][::-1]\n",
    "\n",
    "    # 4. Get the corresponding tags and their scores\n",
    "    top_tags = [(tags_list[i], sim_scores[0][i]) for i in top_indices]\n",
    "    \n",
    "    return top_tags\n",
    "\n",
    "# 3. Call the corrected function, passing in the fitted vectorizer\n",
    "predicted_tags = get_tags_for_description(\n",
    "    new_desc, \n",
    "    unique_tags_list, \n",
    "    tag_embeddings=tag_embeddings, \n",
    "    vectorizer=vectorizer, # Pass the fitted object in!\n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "print(\"Most similar tags for the description:\")\n",
    "for tag, score in predicted_tags:\n",
    "    print(f\"- {tag}: (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queueing up the 5 most similar games for review fetching:\n",
      "- autoheroes (ID: 2403940), Score: 0.6261\n",
      "- auto chess (ID: 1530300), Score: 0.6187\n",
      "- behold battle (ID: 2338910), Score: 0.6037\n",
      "- tiny battles (ID: 2759230), Score: 0.6037\n",
      "- auto riskrisk (ID: 2259990), Score: 0.5962\n",
      "Starting to fetch reviews for 5 games...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aiohttp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     48\u001b[39m   games_to_fetch.append((app_id, game_name))\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 2. Call your existing function to fetch the reviews for just those 5 games\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#    NOTE: This must be run in an async context (e.g., Jupyter or an async function)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m top_5_reviews_data = \u001b[38;5;28;01mawait\u001b[39;00m fetch_all_reviews(games_to_fetch)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 3. Create a new DataFrame from the results\u001b[39;00m\n\u001b[32m     55\u001b[39m reviews_df = pd.DataFrame(top_5_reviews_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mfetch_all_reviews\u001b[39m\u001b[34m(apps_to_fetch)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates and runs all asynchronous tasks for fetching review content.\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting to fetch reviews for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(apps_to_fetch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m games...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43maiohttp\u001b[49m.ClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m     25\u001b[39m     tasks = [fetch_reviews_for_app(session, app_id, name) \u001b[38;5;28;01mfor\u001b[39;00m app_id, name \u001b[38;5;129;01min\u001b[39;00m apps_to_fetch]\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# This will return a list of lists\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'aiohttp' is not defined"
     ]
    }
   ],
   "source": [
    "async def fetch_reviews_for_app(session, app_id, name):\n",
    "    \"\"\"Asynchronously fetches a page of reviews for a single app_id.\"\"\"\n",
    "    url = f\"https://store.steampowered.com/appreviews/{app_id}?json=1&filter=recent&language=english\"\n",
    "    reviews_list = []\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.json()\n",
    "            if data.get(\"success\") == 1 and \"reviews\" in data:\n",
    "                for review_data in data[\"reviews\"]:\n",
    "                    reviews_list.append({\n",
    "                        \"appid\": app_id,\n",
    "                        \"name\": name,\n",
    "                        \"review_text\": review_data.get(\"review\", \"\"),\n",
    "                        \"recommended\": review_data.get(\"voted_up\", False)\n",
    "                    })\n",
    "            return reviews_list\n",
    "    except Exception:\n",
    "        # Return an empty list on error to avoid breaking the process\n",
    "        return []\n",
    "async def fetch_all_reviews(apps_to_fetch):\n",
    "    \"\"\"Creates and runs all asynchronous tasks for fetching review content.\"\"\"\n",
    "    print(f\"Starting to fetch reviews for {len(apps_to_fetch)} games...\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_reviews_for_app(session, app_id, name) for app_id, name in apps_to_fetch]\n",
    "        \n",
    "        # This will return a list of lists\n",
    "        results_list_of_lists = await tqdm.gather(*tasks, desc=\"Fetching reviews\")\n",
    "        \n",
    "        print(\"Finished fetching all reviews. Flattening results...\")\n",
    "        \n",
    "        # Flatten the list of lists into a single list of review dictionaries\n",
    "        all_reviews = [review for sublist in results_list_of_lists for review in sublist]\n",
    "        return all_reviews\n",
    "\n",
    "\n",
    "games_to_fetch = []\n",
    "print(\"Queueing up the 5 most similar games for review fetching:\")\n",
    "\n",
    "for i in top_5_indices:\n",
    "  # Make sure 'appid' and 'name' are the correct column names in your 'steam' DataFrame\n",
    "  game_name = steam.iloc[i]['name']\n",
    "  app_id = steam.iloc[i]['appid'] \n",
    "  \n",
    "  score = similarity_scores[0][i]\n",
    "  print(f\"- {game_name} (ID: {app_id}), Score: {score:.4f}\")\n",
    "  \n",
    "  games_to_fetch.append((app_id, game_name))\n",
    "\n",
    "# 2. Call your existing function to fetch the reviews for just those 5 games\n",
    "#    NOTE: This must be run in an async context (e.g., Jupyter or an async function)\n",
    "top_5_reviews_data = await fetch_all_reviews(games_to_fetch)\n",
    "\n",
    "# 3. Create a new DataFrame from the results\n",
    "reviews_df = pd.DataFrame(top_5_reviews_data)\n",
    "\n",
    "# 4. Display the results\n",
    "print(f\"\\nSuccessfully fetched {len(reviews_df)} reviews for the top 5 games.\")\n",
    "print(reviews_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
